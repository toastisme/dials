# Variables:
#   CACHE_VERSION: unique cache identifier
#   CURRENT_WEEK: weekly changing cache identifier
#   PYTHON_VERSION: string in the form of "3.x"
#   TODAY_ISO: today's date in ISO format, eg. "20200531"
#   isPullRequest: whether this is a pull request build

steps:

# Obtain a shallow clone of the DIALS repository.
# DIALS will not be able to report proper version numbers
- checkout: self
  path: ./modules/dials
  fetchDepth: 1
  displayName: Checkout $(Build.SourceBranch)

# Download source repositories using the bootstrap script
- bash: |
    set -eux
    python modules/dials/installer/bootstrap.py update
  displayName: Repository checkout
  workingDirectory: $(Pipeline.Workspace)

# Create a new conda environment using the bootstrap script
- script: |
    set -eux
    python modules/dials/installer/bootstrap.py base --clean --python $(PYTHON_VERSION)
  displayName: Create python $(PYTHON_VERSION) environment
  workingDirectory: $(Pipeline.Workspace)

# Copy GL/GLU/KHR headers into a place where they can be found during the
# build. We don't include system headers so that we don't accidentally
# pick up libc and friends
- bash: |
    set -eux
    mkdir -p modules/lib build/include
    cp -av /usr/include/GL build/include
    cp -av /usr/include/KHR build/include
    cp -av /usr/lib/x86_64-linux-gnu/libGL.so* modules/lib
    cp -av /usr/lib/x86_64-linux-gnu/libGLU.so* modules/lib
  displayName: Set up GL/GLU libraries and headers
  workingDirectory: $(Pipeline.Workspace)
  condition: and(succeeded(), eq(variables['Agent.OS'], 'Linux'))

# Build DIALS using the bootstrap script
- bash: |
    set -eux
    python modules/dials/installer/bootstrap.py build
  displayName: DIALS build
  workingDirectory: $(Pipeline.Workspace)

# Ensure we are using up-to-date testing packages.
# Extract the dials-data version so we can correctly cache regression data.
- bash: |
    set -e
    . dials
    conda install -y dials-data pytest-azurepipelines pytest-cov pytest-timeout
    set -ux
    dials.data info -v
    echo "##vso[task.setvariable variable=DIALS_DATA_VERSION_FULL]$(dials.data info -v | grep version.full)"
    echo "##vso[task.setvariable variable=DIALS_DATA_VERSION]$(dials.data info -v | grep version.major_minor)"
    mkdir -p data
  displayName: Install additional packages
  workingDirectory: $(Pipeline.Workspace)

# Retrieve the regression data from cache if possible
# The cache allows day-to-day incremental updates, which is relevant only if
# tests are added that refer to datasets in dials-data that were not previously
# referred to.
# New versions of dials-data also lead to cache updates, kick-started from the
# previous cache version.
# The cache is shared across operating systems and python versions, and flushed
# once a week and for dials-data major and minor releases (eg. 2.0->2.1).
# Since dials-data ensures cache coherence this should not be affected by the
# broken Azure cache bug.
- task: Cache@2
  inputs:
    key: '"data" | "$(CACHE_VERSION)-$(CURRENT_WEEK)" | "$(DIALS_DATA_VERSION)" | "$(TODAY_ISO)" | "$(DIALS_DATA_VERSION_FULL)"'
    restoreKeys: |
      "data" | "$(CACHE_VERSION)-$(CURRENT_WEEK)" | "$(DIALS_DATA_VERSION)" | "$(TODAY_ISO)"
      "data" | "$(CACHE_VERSION)-$(CURRENT_WEEK)" | "$(DIALS_DATA_VERSION)"
    path: $(Pipeline.Workspace)/data
    cacheHitVar: DATA_CACHED
  displayName: Restore regression data cache

# Finally, run the full regression test suite
- bash: |
    set -e
    . dials
    set -ux
    export DIALS_DATA=${PWD}/data
    export PYTHONDEVMODE=1
    cd modules/dials
    pytest -v -ra -n auto --basetemp="$(Pipeline.Workspace)/tests" --durations=10 --dist loadgroup\
        --cov=$(pwd) --cov-report=html --cov-report=xml --cov-branch \
        --timeout=5400 --regression || echo "##vso[task.complete result=Failed;]Some tests failed"
  displayName: Run tests
  workingDirectory: $(Pipeline.Workspace)

- bash: bash <(curl -s https://codecov.io/bash) -t $(CODECOV_TOKEN) -n "Python $(PYTHON_VERSION) $(Agent.OS)"
  displayName: Publish coverage stats
  continueOnError: True
  timeoutInMinutes: 2
  workingDirectory: $(Pipeline.Workspace)/modules/dials

# Expand test environment to allow running xfel tests
- bash: |
    set -e
    . dials
    rm -rf tests &  # this will be overwritten with xfel tests
    conda install -y bash distro git-lfs mpi4py openmpi pandas
    cd modules
    git clone https://gitlab.com/cctbx/xfel_regression.git
    cd xfel_regression
    git lfs install --local
    git lfs pull
    libtbx.configure xfel_regression
    wait
  displayName: Prepare xfel_regression tests
  condition: and(eq(variables.isPullRequest, true), succeeded())
  workingDirectory: $(Pipeline.Workspace)

# Finally, run the full regression test suite
- bash: |
    set -e
    . dials
    mkdir -p tests/workdir
    cd tests/workdir
    libtbx.run_tests_parallel module=xfel_regression nproc=auto && {
      echo "XFEL regression tests passed on Python $(PYTHON_VERSION) $(Agent.OS)" > ../xfel-regression.output
    } || {
      echo "##vso[task.complete result=Failed;]XFEL regression tests failed" > ../xfel-regression.output
      echo "##[error]XFEL regression tests failed on Python $(PYTHON_VERSION) $(Agent.OS)" >> ../xfel-regression.output
    }
    mv run_tests_parallel_zlog ../xfel-regression.txt
    cd ..
    rm -rf workdir
  displayName: Run xfel_regression tests
  condition: and(eq(variables.isPullRequest, true), succeeded())
  workingDirectory: $(Pipeline.Workspace)

- task: PublishPipelineArtifact@1
  displayName: Store xfel_regression results
  condition: and(eq(variables.isPullRequest, true), succeeded())
  inputs:
    targetPath: $(Pipeline.Workspace)/tests
    artifactName: xfel-regression - Python $(PYTHON_VERSION) $(Agent.OS) - Run $(System.JobAttempt)

# Reclaim disk space after testing
# This is only relevant if we had cache misses, as free disk space is required to create the dials-data cache archive
- bash: |
    set -eu
    echo -e "\n##[section]Overall disk space usage:"
    df -h
    du -sh *
    echo -e "\n##[section]xfel-regression:"
    du -sh modules/xfel_regression || echo "xfel-regression not present"
    rm -rf modules/xfel_regression
    echo -e "\n##[section]Test artifacts:"
    du -h tests
    rm -rf tests
  displayName: Reclaim disk space
  workingDirectory: $(Pipeline.Workspace)
  condition: and(succeeded(), ne(variables.DATA_CACHED, 'true'))
